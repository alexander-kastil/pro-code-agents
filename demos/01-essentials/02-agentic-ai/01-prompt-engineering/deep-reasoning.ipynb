{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reasoning with Large Language Models\n",
    "\n",
    "This notebook demonstrates **deep reasoning** capabilities using Large Language Models (LLMs). Deep reasoning involves breaking down complex problems into multiple steps, using intermediate reasoning to arrive at solutions, and employing various techniques to enhance the model's analytical and problem-solving abilities.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Deep Reasoning](#introduction)\n",
    "2. [Environment Setup](#setup)\n",
    "3. [Chain-of-Thought Reasoning](#cot)\n",
    "4. [Tree of Thoughts](#tot)\n",
    "5. [Self-Consistency](#self-consistency)\n",
    "6. [Multi-Step Problem Solving](#multi-step)\n",
    "7. [Reasoning with Verification](#verification)\n",
    "8. [Summary and Best Practices](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Deep Reasoning <a id=\"introduction\"></a>\n",
    "\n",
    "### What is Deep Reasoning?\n",
    "\n",
    "**Deep reasoning** refers to the ability of LLMs to:\n",
    "- Break down complex problems into manageable steps\n",
    "- Explore multiple solution paths\n",
    "- Verify and validate intermediate results\n",
    "- Self-reflect on reasoning quality\n",
    "- Adapt strategies when initial approaches fail\n",
    "\n",
    "### Key Techniques\n",
    "\n",
    "1. **Chain-of-Thought (CoT)**: Step-by-step reasoning\n",
    "2. **Tree of Thoughts (ToT)**: Exploring multiple reasoning paths\n",
    "3. **Self-Consistency**: Generating multiple solutions and selecting the most common\n",
    "4. **Least-to-Most Prompting**: Breaking problems into sub-problems\n",
    "5. **Self-Verification**: Having the model check its own work\n",
    "\n",
    "### Benefits of Deep Reasoning\n",
    "\n",
    "- **Improved Accuracy**: Especially for complex problems\n",
    "- **Transparency**: Clear reasoning traces for debugging\n",
    "- **Error Detection**: Self-verification catches mistakes\n",
    "- **Better Generalization**: Works across diverse problem types\n",
    "- **Trust**: Users can follow the model's reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup <a id=\"setup\"></a>\n",
    "\n",
    "Let's set up our connection to Azure AI Foundry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from collections import Counter\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the AI Project client\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=os.environ[\"PROJECT_ENDPOINT\"],\n",
    "    credential=DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "# Get the OpenAI client\n",
    "chat = project_client.get_openai_client()\n",
    "model = os.environ[\"MODEL\"]\n",
    "\n",
    "print(f\"Connected to Azure AI Foundry\")\n",
    "print(f\"Using model: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def get_completion(messages: List[Dict[str, str]], temperature: float = 0.7, max_tokens: int = 500) -> str:\n",
    "    \"\"\"Get completion from the model\"\"\"\n",
    "    response = chat.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def print_section(title: str):\n",
    "    \"\"\"Print formatted section header\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(title)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "print(\"Helper functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chain-of-Thought Reasoning <a id=\"cot\"></a>\n",
    "\n",
    "Chain-of-Thought prompting encourages the model to show its reasoning steps.\n",
    "\n",
    "### 3.1 Basic Chain-of-Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Chain-of-Thought\n",
    "problem = \"\"\"A farmer has 17 sheep. All but 9 die. How many sheep are left?\"\"\"\n",
    "\n",
    "print_section(\"Without Chain-of-Thought\")\n",
    "direct_answer = get_completion(\n",
    "    [{\"role\": \"user\", \"content\": f\"{problem}\\n\\nProvide only the final answer.\"}],\n",
    "    temperature=0.3\n",
    ")\n",
    "print(direct_answer)\n",
    "\n",
    "# With Chain-of-Thought\n",
    "print_section(\"With Chain-of-Thought\")\n",
    "cot_answer = get_completion(\n",
    "    [{\"role\": \"user\", \"content\": f\"{problem}\\n\\nLet's solve this step by step:\"}],\n",
    "    temperature=0.3,\n",
    "    max_tokens=300\n",
    ")\n",
    "print(cot_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Few-Shot Chain-of-Thought\n",
    "\n",
    "Providing examples of step-by-step reasoning improves consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot CoT with examples\n",
    "few_shot_cot = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that solves problems step by step.\"},\n",
    "    {\"role\": \"user\", \"content\": \"A store has 48 apples. They sell 3/4 of them. How many apples are left?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"\"\"Let me solve this step by step:\n",
    "\n",
    "Step 1: Calculate how many apples were sold\n",
    "- They sold 3/4 of 48 apples\n",
    "- 3/4 × 48 = (3 × 48) / 4 = 144 / 4 = 36 apples sold\n",
    "\n",
    "Step 2: Calculate remaining apples\n",
    "- Started with: 48 apples\n",
    "- Sold: 36 apples\n",
    "- Remaining: 48 - 36 = 12 apples\n",
    "\n",
    "Answer: 12 apples are left.\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": \"If a car travels at 60 mph for 2.5 hours, then at 45 mph for 1.5 hours, what is the total distance traveled?\"}\n",
    "]\n",
    "\n",
    "print_section(\"Few-Shot Chain-of-Thought Reasoning\")\n",
    "response = get_completion(few_shot_cot, temperature=0.3, max_tokens=400)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Complex Problem Solving with CoT\n",
    "\n",
    "Let's tackle a more complex problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_problem = \"\"\"A company has three departments: Engineering (60 people), Sales (40 people), and Marketing (30 people).\n",
    "- Engineering is getting a 15% increase in headcount\n",
    "- Sales is getting a 25% increase\n",
    "- Marketing is decreasing by 10%\n",
    "- New hires cost $5,000 each to recruit\n",
    "- Departing employees cost $2,000 each in offboarding\n",
    "\n",
    "What is the total cost for all these changes, and what is the final total headcount?\"\"\"\n",
    "\n",
    "print_section(\"Complex Problem with Detailed CoT\")\n",
    "complex_cot_prompt = f\"{complex_problem}\\n\\nLet's solve this systematically, showing all calculations:\"\n",
    "\n",
    "response = get_completion(\n",
    "    [{\"role\": \"user\", \"content\": complex_cot_prompt}],\n",
    "    temperature=0.2,\n",
    "    max_tokens=600\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tree of Thoughts <a id=\"tot\"></a>\n",
    "\n",
    "Tree of Thoughts explores multiple reasoning paths before selecting the best one.\n",
    "\n",
    "### 4.1 Generating Multiple Reasoning Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_of_thoughts(problem: str, num_paths: int = 3) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate multiple reasoning paths and evaluate them\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    \n",
    "    # Generate multiple reasoning paths\n",
    "    print(\"Generating reasoning paths...\\n\")\n",
    "    for i in range(num_paths):\n",
    "        prompt = f\"\"\"Problem: {problem}\n",
    "\n",
    "Provide a step-by-step solution. Think carefully and show your reasoning.\"\"\"\n",
    "        \n",
    "        response = get_completion(\n",
    "            [{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7,  # Higher temperature for diversity\n",
    "            max_tokens=400\n",
    "        )\n",
    "        \n",
    "        paths.append({\"path_id\": i+1, \"reasoning\": response})\n",
    "        print(f\"Path {i+1}:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(response[:200] + \"...\" if len(response) > 200 else response)\n",
    "        print()\n",
    "    \n",
    "    # Evaluate paths\n",
    "    print(\"\\nEvaluating paths...\\n\")\n",
    "    evaluations = []\n",
    "    \n",
    "    for path in paths:\n",
    "        eval_prompt = f\"\"\"Evaluate this solution on a scale of 1-10 based on:\n",
    "- Logical correctness\n",
    "- Completeness\n",
    "- Clarity of reasoning\n",
    "\n",
    "Solution:\n",
    "{path['reasoning']}\n",
    "\n",
    "Provide ONLY a number from 1-10.\"\"\"\n",
    "        \n",
    "        score_response = get_completion(\n",
    "            [{\"role\": \"user\", \"content\": eval_prompt}],\n",
    "            temperature=0.2,\n",
    "            max_tokens=10\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            score = float(score_response.strip())\n",
    "        except:\n",
    "            score = 5.0\n",
    "        \n",
    "        evaluations.append({\"path_id\": path['path_id'], \"score\": score})\n",
    "        print(f\"Path {path['path_id']} score: {score}/10\")\n",
    "    \n",
    "    # Select best path\n",
    "    best_path = max(evaluations, key=lambda x: x['score'])\n",
    "    best_reasoning = next(p['reasoning'] for p in paths if p['path_id'] == best_path['path_id'])\n",
    "    \n",
    "    return {\n",
    "        \"paths\": paths,\n",
    "        \"evaluations\": evaluations,\n",
    "        \"best_path_id\": best_path['path_id'],\n",
    "        \"best_score\": best_path['score'],\n",
    "        \"best_reasoning\": best_reasoning\n",
    "    }\n",
    "\n",
    "# Test Tree of Thoughts\n",
    "tot_problem = \"\"\"You have 3 boxes: one contains only apples, one contains only oranges, and one contains both.\n",
    "All boxes are labeled incorrectly. You can pick one fruit from one box to determine the correct labels for all boxes.\n",
    "Which box should you pick from and why?\"\"\"\n",
    "\n",
    "print_section(\"Tree of Thoughts Reasoning\")\n",
    "result = tree_of_thoughts(tot_problem, num_paths=3)\n",
    "\n",
    "print_section(\"Best Solution Selected\")\n",
    "print(f\"Path {result['best_path_id']} (Score: {result['best_score']}/10)\")\n",
    "print(\"-\" * 80)\n",
    "print(result['best_reasoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Self-Consistency <a id=\"self-consistency\"></a>\n",
    "\n",
    "Self-consistency generates multiple reasoning paths and selects the most common answer.\n",
    "\n",
    "### 5.1 Multiple Reasoning Paths with Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_consistency(problem: str, num_samples: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate multiple solutions and use majority voting\n",
    "    \"\"\"\n",
    "    solutions = []\n",
    "    final_answers = []\n",
    "    \n",
    "    print(f\"Generating {num_samples} independent solutions...\\n\")\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        prompt = f\"{problem}\\n\\nSolve this step by step and provide your final answer clearly.\"\n",
    "        \n",
    "        response = get_completion(\n",
    "            [{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7,\n",
    "            max_tokens=400\n",
    "        )\n",
    "        \n",
    "        solutions.append(response)\n",
    "        \n",
    "        # Extract final answer (simplified - look for numbers or key phrases)\n",
    "        # In production, use more sophisticated extraction\n",
    "        lines = response.split('\\n')\n",
    "        for line in reversed(lines):\n",
    "            if line.strip():\n",
    "                final_answers.append(line.strip())\n",
    "                break\n",
    "        \n",
    "        print(f\"Solution {i+1}: {response[:150]}...\")\n",
    "        print()\n",
    "    \n",
    "    # Find most common answer\n",
    "    answer_counts = Counter(final_answers)\n",
    "    most_common_answer, count = answer_counts.most_common(1)[0]\n",
    "    \n",
    "    return {\n",
    "        \"solutions\": solutions,\n",
    "        \"final_answers\": final_answers,\n",
    "        \"most_common_answer\": most_common_answer,\n",
    "        \"confidence\": count / num_samples,\n",
    "        \"agreement_count\": count,\n",
    "        \"total_samples\": num_samples\n",
    "    }\n",
    "\n",
    "# Test self-consistency\n",
    "sc_problem = \"\"\"A bat and a ball together cost $1.10.\n",
    "The bat costs $1.00 more than the ball.\n",
    "How much does the ball cost?\"\"\"\n",
    "\n",
    "print_section(\"Self-Consistency Approach\")\n",
    "result = self_consistency(sc_problem, num_samples=5)\n",
    "\n",
    "print_section(\"Self-Consistency Results\")\n",
    "print(f\"Most Common Answer: {result['most_common_answer']}\")\n",
    "print(f\"Confidence: {result['confidence']:.0%} ({result['agreement_count']}/{result['total_samples']} solutions agree)\")\n",
    "print(\"\\nAll final answers:\")\n",
    "for i, answer in enumerate(result['final_answers'], 1):\n",
    "    print(f\"{i}. {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Step Problem Solving <a id=\"multi-step\"></a>\n",
    "\n",
    "### 6.1 Least-to-Most Prompting\n",
    "\n",
    "Break complex problems into simpler sub-problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_to_most_prompting(problem: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Break problem into sub-problems and solve incrementally\n",
    "    \"\"\"\n",
    "    # Step 1: Decompose the problem\n",
    "    print(\"Step 1: Problem Decomposition\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    decompose_prompt = f\"\"\"Break down this complex problem into 3-4 simpler sub-problems that need to be solved.\n",
    "\n",
    "Problem: {problem}\n",
    "\n",
    "List the sub-problems in order:\"\"\"\n",
    "    \n",
    "    decomposition = get_completion(\n",
    "        [{\"role\": \"user\", \"content\": decompose_prompt}],\n",
    "        temperature=0.3,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    print(decomposition)\n",
    "    print()\n",
    "    \n",
    "    # Parse sub-problems\n",
    "    sub_problems = []\n",
    "    for line in decomposition.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if line and (line[0].isdigit() or line.startswith('-')):\n",
    "            clean = line.lstrip('0123456789.-) ').strip()\n",
    "            if clean:\n",
    "                sub_problems.append(clean)\n",
    "    \n",
    "    # Step 2: Solve sub-problems incrementally\n",
    "    print(\"\\nStep 2: Solving Sub-Problems\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    solutions = []\n",
    "    context = \"\"\n",
    "    \n",
    "    for i, sub_problem in enumerate(sub_problems[:4], 1):\n",
    "        print(f\"\\nSub-problem {i}: {sub_problem}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        solve_prompt = f\"\"\"Previous context:\n",
    "{context}\n",
    "\n",
    "Now solve this sub-problem:\n",
    "{sub_problem}\n",
    "\n",
    "Provide a concise solution:\"\"\"\n",
    "        \n",
    "        solution = get_completion(\n",
    "            [{\"role\": \"user\", \"content\": solve_prompt}],\n",
    "            temperature=0.3,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        solutions.append({\"sub_problem\": sub_problem, \"solution\": solution})\n",
    "        context += f\"\\n\\nSub-problem {i}: {sub_problem}\\nSolution: {solution}\"\n",
    "        \n",
    "        print(solution)\n",
    "    \n",
    "    # Step 3: Synthesize final answer\n",
    "    print(\"\\n\\nStep 3: Synthesizing Final Answer\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    synthesize_prompt = f\"\"\"Based on the solutions to these sub-problems, provide a comprehensive final answer to the original problem.\n",
    "\n",
    "Original Problem: {problem}\n",
    "\n",
    "Sub-problem Solutions:\n",
    "{context}\n",
    "\n",
    "Final comprehensive answer:\"\"\"\n",
    "    \n",
    "    final_answer = get_completion(\n",
    "        [{\"role\": \"user\", \"content\": synthesize_prompt}],\n",
    "        temperature=0.3,\n",
    "        max_tokens=400\n",
    "    )\n",
    "    \n",
    "    print(final_answer)\n",
    "    \n",
    "    return {\n",
    "        \"decomposition\": decomposition,\n",
    "        \"sub_problems\": sub_problems,\n",
    "        \"solutions\": solutions,\n",
    "        \"final_answer\": final_answer\n",
    "    }\n",
    "\n",
    "# Test least-to-most prompting\n",
    "ltm_problem = \"\"\"Design a cloud-native microservices architecture for an e-commerce platform that needs to:\n",
    "- Handle 10,000 concurrent users\n",
    "- Process payments securely\n",
    "- Manage inventory across multiple warehouses\n",
    "- Provide real-time order tracking\n",
    "- Scale automatically during peak shopping periods\n",
    "\n",
    "What are the key architectural components and how should they interact?\"\"\"\n",
    "\n",
    "print_section(\"Least-to-Most Prompting\")\n",
    "result = least_to_most_prompting(ltm_problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reasoning with Verification <a id=\"verification\"></a>\n",
    "\n",
    "### 7.1 Self-Verification\n",
    "\n",
    "The model checks its own work for errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reasoning_with_verification(problem: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Solve problem, then verify the solution\n",
    "    \"\"\"\n",
    "    # Step 1: Generate initial solution\n",
    "    print(\"Step 1: Initial Solution\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    solve_prompt = f\"{problem}\\n\\nSolve this step by step:\"\n",
    "    \n",
    "    initial_solution = get_completion(\n",
    "        [{\"role\": \"user\", \"content\": solve_prompt}],\n",
    "        temperature=0.3,\n",
    "        max_tokens=400\n",
    "    )\n",
    "    print(initial_solution)\n",
    "    print()\n",
    "    \n",
    "    # Step 2: Verify the solution\n",
    "    print(\"\\nStep 2: Verification\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    verify_prompt = f\"\"\"Review this solution and check for errors:\n",
    "\n",
    "Problem: {problem}\n",
    "\n",
    "Solution:\n",
    "{initial_solution}\n",
    "\n",
    "Verify:\n",
    "1. Are all calculations correct?\n",
    "2. Is the logic sound?\n",
    "3. Are there any mistakes or oversights?\n",
    "\n",
    "Provide your verification:\"\"\"\n",
    "    \n",
    "    verification = get_completion(\n",
    "        [{\"role\": \"user\", \"content\": verify_prompt}],\n",
    "        temperature=0.2,\n",
    "        max_tokens=400\n",
    "    )\n",
    "    print(verification)\n",
    "    print()\n",
    "    \n",
    "    # Step 3: Correct if needed\n",
    "    print(\"\\nStep 3: Correction (if needed)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    if \"error\" in verification.lower() or \"mistake\" in verification.lower() or \"incorrect\" in verification.lower():\n",
    "        correct_prompt = f\"\"\"The verification found issues with the solution.\n",
    "\n",
    "Original Solution:\n",
    "{initial_solution}\n",
    "\n",
    "Verification Feedback:\n",
    "{verification}\n",
    "\n",
    "Provide a corrected solution:\"\"\"\n",
    "        \n",
    "        corrected_solution = get_completion(\n",
    "            [{\"role\": \"user\", \"content\": correct_prompt}],\n",
    "            temperature=0.2,\n",
    "            max_tokens=400\n",
    "        )\n",
    "        print(\"Issues found. Corrected solution:\")\n",
    "        print(corrected_solution)\n",
    "        final_solution = corrected_solution\n",
    "    else:\n",
    "        print(\"No issues found. Original solution is correct.\")\n",
    "        final_solution = initial_solution\n",
    "    \n",
    "    return {\n",
    "        \"initial_solution\": initial_solution,\n",
    "        \"verification\": verification,\n",
    "        \"final_solution\": final_solution,\n",
    "        \"corrected\": \"error\" in verification.lower() or \"mistake\" in verification.lower()\n",
    "    }\n",
    "\n",
    "# Test reasoning with verification\n",
    "verify_problem = \"\"\"Calculate the compound interest on $5,000 invested at 6% annual interest rate,\n",
    "compounded quarterly, for 3 years. Show your work.\"\"\"\n",
    "\n",
    "print_section(\"Reasoning with Self-Verification\")\n",
    "result = reasoning_with_verification(verify_problem)\n",
    "\n",
    "print_section(\"Summary\")\n",
    "print(f\"Correction needed: {'Yes' if result['corrected'] else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Multi-Agent Verification\n",
    "\n",
    "Use different \"agents\" with different perspectives to verify solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_agent_verification(problem: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Have multiple 'agents' with different roles verify the solution\n",
    "    \"\"\"\n",
    "    # Generate initial solution\n",
    "    initial_solution = get_completion(\n",
    "        [{\"role\": \"user\", \"content\": f\"{problem}\\n\\nSolve this step by step:\"}],\n",
    "        temperature=0.3,\n",
    "        max_tokens=400\n",
    "    )\n",
    "    \n",
    "    print(\"Initial Solution:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(initial_solution)\n",
    "    print()\n",
    "    \n",
    "    # Different verification perspectives\n",
    "    agents = [\n",
    "        {\n",
    "            \"role\": \"Mathematical Rigor Checker\",\n",
    "            \"prompt\": \"Check if all mathematical calculations and formulas are correct.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"Logical Consistency Checker\",\n",
    "            \"prompt\": \"Verify that the logical flow and reasoning steps are sound.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"Assumptions Validator\",\n",
    "            \"prompt\": \"Identify any assumptions made and verify they are reasonable and stated.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    verifications = []\n",
    "    \n",
    "    print(\"\\nMulti-Agent Verification:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for agent in agents:\n",
    "        print(f\"\\n{agent['role']}:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        verify_prompt = f\"\"\"You are a {agent['role']}.\n",
    "\n",
    "Problem: {problem}\n",
    "\n",
    "Solution to verify:\n",
    "{initial_solution}\n",
    "\n",
    "Task: {agent['prompt']}\n",
    "Provide your assessment:\"\"\"\n",
    "        \n",
    "        verification = get_completion(\n",
    "            [{\"role\": \"user\", \"content\": verify_prompt}],\n",
    "            temperature=0.2,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        verifications.append({\n",
    "            \"agent\": agent['role'],\n",
    "            \"assessment\": verification\n",
    "        })\n",
    "        \n",
    "        print(verification)\n",
    "    \n",
    "    # Synthesize all verifications\n",
    "    print(\"\\n\\nFinal Synthesis:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    all_assessments = \"\\n\\n\".join([f\"{v['agent']}:\\n{v['assessment']}\" for v in verifications])\n",
    "    \n",
    "    synthesis_prompt = f\"\"\"Based on these verification assessments, provide a final verdict on the solution:\n",
    "\n",
    "{all_assessments}\n",
    "\n",
    "Is the solution correct? If not, what needs to be corrected?\"\"\"\n",
    "    \n",
    "    final_verdict = get_completion(\n",
    "        [{\"role\": \"user\", \"content\": synthesis_prompt}],\n",
    "        temperature=0.2,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    print(final_verdict)\n",
    "    \n",
    "    return {\n",
    "        \"initial_solution\": initial_solution,\n",
    "        \"verifications\": verifications,\n",
    "        \"final_verdict\": final_verdict\n",
    "    }\n",
    "\n",
    "# Test multi-agent verification\n",
    "ma_problem = \"\"\"A rectangular garden is 20 meters long and 15 meters wide.\n",
    "If you want to put a fence around it that costs $12 per meter, and also need\n",
    "to leave a 3-meter opening for a gate, what is the total cost?\"\"\"\n",
    "\n",
    "print_section(\"Multi-Agent Verification\")\n",
    "result = multi_agent_verification(ma_problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Best Practices <a id=\"summary\"></a>\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**Deep Reasoning Techniques:**\n",
    "\n",
    "1. **Chain-of-Thought**: Essential for complex reasoning tasks\n",
    "   - Use \"Let's solve this step by step\"\n",
    "   - Provide examples (few-shot) for consistency\n",
    "   - Lower temperature (0.2-0.4) for factual accuracy\n",
    "\n",
    "2. **Tree of Thoughts**: For problems with multiple valid approaches\n",
    "   - Generate diverse paths with higher temperature\n",
    "   - Evaluate and select best path\n",
    "   - More API calls but better quality\n",
    "\n",
    "3. **Self-Consistency**: Improves accuracy through voting\n",
    "   - Generate 3-5 independent solutions\n",
    "   - Use majority voting for final answer\n",
    "   - Confidence = agreement percentage\n",
    "\n",
    "4. **Least-to-Most**: For complex, decomposable problems\n",
    "   - Break into sub-problems\n",
    "   - Solve incrementally with context\n",
    "   - Synthesize final answer\n",
    "\n",
    "5. **Self-Verification**: Catch errors automatically\n",
    "   - Verify calculations and logic\n",
    "   - Multi-agent verification for thorough checking\n",
    "   - Iterate until correct\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "**When to Use Each Technique:**\n",
    "\n",
    "| Technique | Best For | Cost | Accuracy Gain |\n",
    "|-----------|----------|------|---------------|\n",
    "| Basic CoT | Most reasoning tasks | Low | Medium |\n",
    "| Few-Shot CoT | Consistent formatting needed | Low | High |\n",
    "| Tree of Thoughts | Multiple valid approaches | High | Very High |\n",
    "| Self-Consistency | High-stakes decisions | High | High |\n",
    "| Least-to-Most | Complex decomposable problems | Medium | High |\n",
    "| Self-Verification | Error-sensitive applications | Medium | Very High |\n",
    "\n",
    "**Implementation Tips:**\n",
    "\n",
    "1. **Start Simple**: Begin with basic CoT, add complexity as needed\n",
    "2. **Adjust Temperature**: \n",
    "   - Low (0.2-0.3) for factual accuracy\n",
    "   - Medium (0.5-0.7) for diverse reasoning paths\n",
    "3. **Set Token Limits**: Allow enough for detailed reasoning (400-600 tokens)\n",
    "4. **Monitor Costs**: Deep reasoning uses more API calls\n",
    "5. **Cache Results**: Store intermediate reasoning for efficiency\n",
    "6. **Measure Impact**: A/B test to verify accuracy improvements\n",
    "\n",
    "**Combining Techniques:**\n",
    "\n",
    "For maximum accuracy:\n",
    "1. Use Least-to-Most to decompose complex problems\n",
    "2. Apply Self-Consistency to each sub-problem\n",
    "3. Use Self-Verification on the final answer\n",
    "4. Keep reasoning traces for transparency and debugging\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "1. **Latency vs. Accuracy**: Deep reasoning increases response time\n",
    "2. **Cost Management**: Multiple API calls add up quickly\n",
    "3. **Caching**: Cache reasoning for similar questions\n",
    "4. **Monitoring**: Track reasoning quality and success rates\n",
    "5. **Fallbacks**: Have simpler methods if deep reasoning fails\n",
    "6. **User Experience**: Show reasoning traces to build trust\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore domain-specific reasoning (code, math, science)\n",
    "- Integrate with tool usage for enhanced capabilities\n",
    "- Build evaluation harnesses to measure improvements\n",
    "- Combine with RAG for knowledge-grounded reasoning\n",
    "- Implement reasoning traces in production systems\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903)\n",
    "- [Tree of Thoughts](https://arxiv.org/abs/2305.10601)\n",
    "- [Self-Consistency Improves Chain of Thought](https://arxiv.org/abs/2203.11171)\n",
    "- [Least-to-Most Prompting](https://arxiv.org/abs/2205.10625)\n",
    "- [Azure AI Foundry Documentation](https://learn.microsoft.com/azure/ai-studio/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "Try implementing these challenges:\n",
    "\n",
    "1. **Hybrid Reasoning**: Combine Tree of Thoughts with Self-Consistency\n",
    "2. **Domain-Specific**: Apply deep reasoning to code debugging or mathematical proofs\n",
    "3. **Iterative Refinement**: Build a system that refines reasoning based on verification feedback\n",
    "4. **Explainable AI**: Create reasoning traces that non-technical users can understand\n",
    "5. **Multi-Modal Reasoning**: Extend to problems involving images or structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your practice code here\n",
    "# Try implementing one of the challenges above!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
