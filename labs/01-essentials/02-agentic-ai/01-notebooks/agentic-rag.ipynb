{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "This notebook demonstrates how to build an **Agentic RAG system** that combines retrieval-augmented generation with autonomous agent capabilities. Unlike traditional RAG systems that follow a simple retrieve-then-generate pattern, agentic RAG systems can dynamically decide when and how to retrieve information, reformulate queries, and iteratively refine their responses.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Agentic RAG](#introduction)\n",
    "2. [Environment Setup](#setup)\n",
    "3. [Traditional RAG vs. Agentic RAG](#comparison)\n",
    "4. [Building an Agentic RAG System](#building)\n",
    "5. [Advanced Agentic RAG Patterns](#advanced)\n",
    "6. [Summary and Best Practices](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Agentic RAG <a id=\"introduction\"></a>\n",
    "\n",
    "### What is Agentic RAG?\n",
    "\n",
    "**Agentic RAG** extends traditional Retrieval-Augmented Generation by giving the system autonomous decision-making capabilities:\n",
    "\n",
    "- **Dynamic Query Planning**: The agent decides when and what to retrieve\n",
    "- **Self-Reflection**: The agent evaluates whether retrieved information is sufficient\n",
    "- **Query Refinement**: The agent reformulates queries to get better results\n",
    "- **Multi-Step Reasoning**: The agent breaks down complex questions into sub-queries\n",
    "- **Adaptive Retrieval**: The agent adjusts retrieval strategies based on context\n",
    "\n",
    "### Key Differences from Traditional RAG\n",
    "\n",
    "| Traditional RAG | Agentic RAG |\n",
    "|----------------|-------------|\n",
    "| Fixed retrieve-then-generate flow | Dynamic decision-making about retrieval |\n",
    "| Single retrieval step | Multiple adaptive retrieval steps |\n",
    "| No query reformulation | Intelligent query refinement |\n",
    "| Limited error handling | Self-correction and re-retrieval |\n",
    "| Static context usage | Context-aware strategy selection |\n",
    "\n",
    "### When to Use Agentic RAG\n",
    "\n",
    "Agentic RAG is ideal for:\n",
    "- Complex multi-step questions requiring information synthesis\n",
    "- Scenarios where initial retrieval may be insufficient\n",
    "- Cases requiring query disambiguation or refinement\n",
    "- Applications needing high-quality, verified responses\n",
    "- Systems that benefit from explanatory reasoning traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup <a id=\"setup\"></a>\n",
    "\n",
    "First, let's set up our connection to Azure AI Foundry and create a simple in-memory knowledge base for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the AI Project client\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=os.environ[\"PROJECT_ENDPOINT\"],\n",
    "    credential=DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "# Get the OpenAI client\n",
    "chat = project_client.get_openai_client()\n",
    "model = os.environ[\"MODEL\"]\n",
    "\n",
    "print(f\"Connected to Azure AI Foundry\")\n",
    "print(f\"Using model: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple in-memory knowledge base for demonstration\n",
    "# In production, this would be Azure AI Search, vector database, etc.\n",
    "\n",
    "knowledge_base = [\n",
    "    {\n",
    "        \"id\": \"doc1\",\n",
    "        \"title\": \"Azure AI Foundry Overview\",\n",
    "        \"content\": \"Azure AI Foundry is a comprehensive platform for building, deploying, and managing AI applications. It provides tools for prompt engineering, model deployment, evaluation, and monitoring.\",\n",
    "        \"category\": \"platform\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc2\",\n",
    "        \"title\": \"RAG Systems Explained\",\n",
    "        \"content\": \"Retrieval-Augmented Generation (RAG) combines information retrieval with text generation. It retrieves relevant documents from a knowledge base and uses them to generate accurate, grounded responses.\",\n",
    "        \"category\": \"rag\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc3\",\n",
    "        \"title\": \"Agent Frameworks\",\n",
    "        \"content\": \"Microsoft Agent Framework enables building autonomous agents that can use tools, make decisions, and execute complex workflows. Agents can plan, reflect, and adapt their behavior based on context.\",\n",
    "        \"category\": \"agents\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc4\",\n",
    "        \"title\": \"Vector Embeddings\",\n",
    "        \"content\": \"Vector embeddings are numerical representations of text that capture semantic meaning. They enable similarity search by finding documents with similar meaning rather than just matching keywords.\",\n",
    "        \"category\": \"embeddings\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc5\",\n",
    "        \"title\": \"Prompt Engineering Best Practices\",\n",
    "        \"content\": \"Effective prompt engineering involves being specific, providing context, using examples (few-shot learning), and structuring prompts clearly. Chain-of-thought prompting helps with complex reasoning tasks.\",\n",
    "        \"category\": \"prompts\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc6\",\n",
    "        \"title\": \"Azure AI Search Integration\",\n",
    "        \"content\": \"Azure AI Search provides powerful indexing and retrieval capabilities. It supports vector search, hybrid search, and semantic ranking for improved relevance in RAG applications.\",\n",
    "        \"category\": \"search\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc7\",\n",
    "        \"title\": \"Model Evaluation Techniques\",\n",
    "        \"content\": \"Evaluating AI models involves metrics like relevance, groundedness, coherence, and fluency. Azure AI Foundry provides built-in evaluation tools to measure model performance systematically.\",\n",
    "        \"category\": \"evaluation\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc8\",\n",
    "        \"title\": \"Agent Tool Usage\",\n",
    "        \"content\": \"Agents can use various tools including search APIs, calculators, code interpreters, and custom functions. Tool selection and usage is a key capability that distinguishes agents from simple chatbots.\",\n",
    "        \"category\": \"agents\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(knowledge_base)} documents into knowledge base\")\n",
    "print(\"\\nSample documents:\")\n",
    "for doc in knowledge_base[:3]:\n",
    "    print(f\"- {doc['title']} ({doc['category']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for simple keyword-based retrieval (simplified for demo)\n",
    "# In production, use semantic search with embeddings\n",
    "\n",
    "def simple_retrieve(query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Simple keyword-based retrieval for demonstration\"\"\"\n",
    "    query_lower = query.lower()\n",
    "    scored_docs = []\n",
    "    \n",
    "    for doc in knowledge_base:\n",
    "        # Simple scoring based on keyword matches\n",
    "        score = 0\n",
    "        content_lower = (doc['title'] + ' ' + doc['content']).lower()\n",
    "        \n",
    "        for word in query_lower.split():\n",
    "            if len(word) > 3:  # Only count meaningful words\n",
    "                score += content_lower.count(word)\n",
    "        \n",
    "        scored_docs.append((score, doc))\n",
    "    \n",
    "    # Sort by score and return top_k\n",
    "    scored_docs.sort(reverse=True, key=lambda x: x[0])\n",
    "    return [doc for score, doc in scored_docs[:top_k] if score > 0]\n",
    "\n",
    "# Test retrieval\n",
    "test_query = \"How does RAG work?\"\n",
    "retrieved = simple_retrieve(test_query)\n",
    "print(f\"Retrieved {len(retrieved)} documents for query: '{test_query}'\")\n",
    "for doc in retrieved:\n",
    "    print(f\"- {doc['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Traditional RAG vs. Agentic RAG <a id=\"comparison\"></a>\n",
    "\n",
    "Let's compare how traditional RAG and agentic RAG handle the same question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional RAG: Simple retrieve-then-generate\n",
    "\n",
    "def traditional_rag(query: str) -> str:\n",
    "    \"\"\"Traditional RAG: retrieve once, then generate\"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    retrieved_docs = simple_retrieve(query, top_k=2)\n",
    "    \n",
    "    # Step 2: Build context from retrieved documents\n",
    "    context = \"\\n\\n\".join([f\"{doc['title']}:\\n{doc['content']}\" for doc in retrieved_docs])\n",
    "    \n",
    "    # Step 3: Generate response using context\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Use the provided context to answer questions accurately.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer based on the context provided:\"}\n",
    "    ]\n",
    "    \n",
    "    response = chat.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.3,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test traditional RAG\n",
    "question = \"How can I build an intelligent agent that uses search capabilities?\"\n",
    "print(\"Traditional RAG Response:\")\n",
    "print(\"=\" * 80)\n",
    "response = traditional_rag(question)\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agentic RAG: Intelligent multi-step retrieval with reasoning\n",
    "\n",
    "def agentic_rag(query: str, max_iterations: int = 3) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Agentic RAG with self-reflection and iterative refinement.\n",
    "    Returns both the answer and the reasoning trace.\n",
    "    \"\"\"\n",
    "    reasoning_trace = []\n",
    "    all_retrieved_docs = []\n",
    "    \n",
    "    # Step 1: Agent analyzes the query and plans retrieval strategy\n",
    "    planning_prompt = f\"\"\"Analyze this question and break it down into key information needs:\n",
    "Question: {query}\n",
    "\n",
    "Provide:\n",
    "1. Main topic/concept\n",
    "2. Specific information required\n",
    "3. Suggested search queries (2-3 focused queries)\n",
    "\n",
    "Format your response as JSON with keys: main_topic, info_needed, search_queries\"\"\"\n",
    "    \n",
    "    planning_response = chat.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": planning_prompt}],\n",
    "        temperature=0.3,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    plan_text = planning_response.choices[0].message.content\n",
    "    reasoning_trace.append({\"step\": \"planning\", \"content\": plan_text})\n",
    "    \n",
    "    # Extract search queries (simplified - in production use JSON parsing)\n",
    "    # For demo, we'll use the original query and a refined version\n",
    "    search_queries = [query, query.replace(\"?\", \"\")]\n",
    "    \n",
    "    # Step 2: Iterative retrieval with self-reflection\n",
    "    for iteration in range(max_iterations):\n",
    "        # Retrieve documents\n",
    "        for sq in search_queries[:1]:  # Use first query for iteration\n",
    "            docs = simple_retrieve(sq, top_k=2)\n",
    "            all_retrieved_docs.extend(docs)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        seen_ids = set()\n",
    "        unique_docs = []\n",
    "        for doc in all_retrieved_docs:\n",
    "            if doc['id'] not in seen_ids:\n",
    "                unique_docs.append(doc)\n",
    "                seen_ids.add(doc['id'])\n",
    "        all_retrieved_docs = unique_docs\n",
    "        \n",
    "        # Build context\n",
    "        context = \"\\n\\n\".join([f\"{doc['title']}:\\n{doc['content']}\" for doc in all_retrieved_docs])\n",
    "        \n",
    "        # Step 3: Self-reflection - is the information sufficient?\n",
    "        reflection_prompt = f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Evaluate if the context provides sufficient information to answer the question comprehensively.\n",
    "Response with ONLY 'SUFFICIENT' or 'INSUFFICIENT: <reason>'\"\"\"\n",
    "        \n",
    "        reflection_response = chat.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": reflection_prompt}],\n",
    "            temperature=0.2,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        \n",
    "        reflection = reflection_response.choices[0].message.content\n",
    "        reasoning_trace.append({\"step\": f\"reflection_{iteration+1}\", \"content\": reflection})\n",
    "        \n",
    "        if \"SUFFICIENT\" in reflection.upper():\n",
    "            break\n",
    "    \n",
    "    # Step 4: Generate final answer with reasoning\n",
    "    final_prompt = f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Provide a comprehensive answer based on the context. Be specific and cite relevant information.\"\"\"\n",
    "    \n",
    "    final_response = chat.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": final_prompt}],\n",
    "        temperature=0.3,\n",
    "        max_tokens=400\n",
    "    )\n",
    "    \n",
    "    answer = final_response.choices[0].message.content\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"reasoning_trace\": reasoning_trace,\n",
    "        \"retrieved_docs\": [doc['title'] for doc in all_retrieved_docs],\n",
    "        \"num_iterations\": len([t for t in reasoning_trace if 'reflection' in t['step']])\n",
    "    }\n",
    "\n",
    "# Test agentic RAG\n",
    "print(\"Agentic RAG Response:\")\n",
    "print(\"=\" * 80)\n",
    "result = agentic_rag(question)\n",
    "print(f\"Answer:\\n{result['answer']}\")\n",
    "print(f\"\\nRetrieved Documents: {', '.join(result['retrieved_docs'])}\")\n",
    "print(f\"Iterations: {result['num_iterations']}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Observations:**\n",
    "\n",
    "- **Traditional RAG**: Single retrieval, direct answer generation\n",
    "- **Agentic RAG**: Query planning, iterative retrieval, self-reflection, comprehensive answer\n",
    "\n",
    "The agentic approach provides:\n",
    "1. Better query understanding and decomposition\n",
    "2. Self-verification of information sufficiency\n",
    "3. Ability to retrieve additional information if needed\n",
    "4. Transparent reasoning trace for debugging and trust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building an Agentic RAG System <a id=\"building\"></a>\n",
    "\n",
    "Let's build a more sophisticated agentic RAG system with additional capabilities.\n",
    "\n",
    "### 4.1 Query Decomposition\n",
    "\n",
    "For complex questions, the agent breaks them into sub-queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_query(complex_query: str) -> List[str]:\n",
    "    \"\"\"Break down complex queries into simpler sub-queries\"\"\"\n",
    "    decomposition_prompt = f\"\"\"Break down this complex question into 2-3 simpler sub-questions that, when answered, would provide a complete answer to the original question.\n",
    "\n",
    "Complex Question: {complex_query}\n",
    "\n",
    "Provide sub-questions as a numbered list.\"\"\"\n",
    "    \n",
    "    response = chat.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": decomposition_prompt}],\n",
    "        temperature=0.3,\n",
    "        max_tokens=200\n",
    "    )\n",
    "    \n",
    "    decomposition = response.choices[0].message.content\n",
    "    \n",
    "    # Parse sub-questions (simplified)\n",
    "    sub_queries = []\n",
    "    for line in decomposition.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if line and (line[0].isdigit() or line.startswith('-')):\n",
    "            # Remove numbering and clean up\n",
    "            clean_line = line.lstrip('0123456789.-) ').strip()\n",
    "            if clean_line:\n",
    "                sub_queries.append(clean_line)\n",
    "    \n",
    "    return sub_queries if sub_queries else [complex_query]\n",
    "\n",
    "# Test query decomposition\n",
    "complex_q = \"What are the key differences between traditional RAG and agentic RAG, and how do vector embeddings improve retrieval quality?\"\n",
    "print(f\"Original Question:\\n{complex_q}\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Decomposed Sub-Questions:\")\n",
    "sub_qs = decompose_query(complex_q)\n",
    "for i, sq in enumerate(sub_qs, 1):\n",
    "    print(f\"{i}. {sq}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Query Reformulation\n",
    "\n",
    "The agent can reformulate queries to improve retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformulate_query(original_query: str, context: str = \"\") -> List[str]:\n",
    "    \"\"\"Generate alternative formulations of a query for better retrieval\"\"\"\n",
    "    reformulation_prompt = f\"\"\"Generate 2-3 alternative phrasings of this question that might retrieve different relevant information.\n",
    "\n",
    "Original Question: {original_query}\n",
    "{f'Previous Context: {context}' if context else ''}\n",
    "\n",
    "Provide alternatives as a numbered list. Make them specific and diverse.\"\"\"\n",
    "    \n",
    "    response = chat.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": reformulation_prompt}],\n",
    "        temperature=0.5,\n",
    "        max_tokens=200\n",
    "    )\n",
    "    \n",
    "    reformulations_text = response.choices[0].message.content\n",
    "    \n",
    "    # Parse reformulations\n",
    "    reformulations = [original_query]  # Include original\n",
    "    for line in reformulations_text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if line and (line[0].isdigit() or line.startswith('-')):\n",
    "            clean_line = line.lstrip('0123456789.-) ').strip()\n",
    "            if clean_line:\n",
    "                reformulations.append(clean_line)\n",
    "    \n",
    "    return reformulations[:4]  # Original + 3 alternatives\n",
    "\n",
    "# Test query reformulation\n",
    "test_q = \"How do agents use tools?\"\n",
    "print(f\"Original Query: {test_q}\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Reformulated Queries:\")\n",
    "reformulated = reformulate_query(test_q)\n",
    "for i, rq in enumerate(reformulated, 1):\n",
    "    print(f\"{i}. {rq}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Complete Agentic RAG with All Features\n",
    "\n",
    "Now let's put it all together into a comprehensive agentic RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_agentic_rag(query: str, verbose: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Advanced Agentic RAG with:\n",
    "    - Query decomposition for complex questions\n",
    "    - Query reformulation for better retrieval\n",
    "    - Iterative retrieval with self-reflection\n",
    "    - Reasoning trace for transparency\n",
    "    \"\"\"\n",
    "    trace = []\n",
    "    all_docs = []\n",
    "    \n",
    "    # Step 1: Decompose complex query\n",
    "    if verbose:\n",
    "        print(\"Step 1: Query Decomposition\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    sub_queries = decompose_query(query)\n",
    "    trace.append({\"step\": \"decomposition\", \"sub_queries\": sub_queries})\n",
    "    \n",
    "    if verbose:\n",
    "        for i, sq in enumerate(sub_queries, 1):\n",
    "            print(f\"  {i}. {sq}\")\n",
    "        print()\n",
    "    \n",
    "    # Step 2: For each sub-query, reformulate and retrieve\n",
    "    if verbose:\n",
    "        print(\"Step 2: Query Reformulation & Retrieval\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    for i, sub_q in enumerate(sub_queries[:2]):  # Limit to 2 sub-queries for demo\n",
    "        # Reformulate\n",
    "        reformulated = reformulate_query(sub_q)\n",
    "        if verbose:\n",
    "            print(f\"  Sub-query {i+1}: {sub_q}\")\n",
    "            print(f\"  Reformulations: {len(reformulated)}\")\n",
    "        \n",
    "        # Retrieve for each reformulation\n",
    "        for ref_q in reformulated[:2]:  # Use first 2 reformulations\n",
    "            docs = simple_retrieve(ref_q, top_k=2)\n",
    "            all_docs.extend(docs)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Retrieved {len(docs)} documents\")\n",
    "            print()\n",
    "    \n",
    "    # Remove duplicates\n",
    "    seen_ids = set()\n",
    "    unique_docs = []\n",
    "    for doc in all_docs:\n",
    "        if doc['id'] not in seen_ids:\n",
    "            unique_docs.append(doc)\n",
    "            seen_ids.add(doc['id'])\n",
    "    \n",
    "    trace.append({\"step\": \"retrieval\", \"num_docs\": len(unique_docs)})\n",
    "    \n",
    "    # Step 3: Self-reflection on information quality\n",
    "    if verbose:\n",
    "        print(\"Step 3: Self-Reflection\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    context = \"\\n\\n\".join([f\"{doc['title']}:\\n{doc['content']}\" for doc in unique_docs])\n",
    "    \n",
    "    reflection_prompt = f\"\"\"Evaluate if the following context provides sufficient information to answer this question comprehensively.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Provide:\n",
    "1. Assessment (SUFFICIENT or INSUFFICIENT)\n",
    "2. Brief reasoning\n",
    "3. If insufficient, what information is missing?\"\"\"\n",
    "    \n",
    "    reflection_response = chat.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": reflection_prompt}],\n",
    "        temperature=0.2,\n",
    "        max_tokens=200\n",
    "    )\n",
    "    \n",
    "    reflection = reflection_response.choices[0].message.content\n",
    "    trace.append({\"step\": \"reflection\", \"assessment\": reflection})\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  {reflection}\")\n",
    "        print()\n",
    "    \n",
    "    # Step 4: Generate comprehensive answer\n",
    "    if verbose:\n",
    "        print(\"Step 4: Answer Generation\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    answer_prompt = f\"\"\"Based on the following context, provide a comprehensive answer to the question.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Provide a detailed, well-structured answer. Cite specific information from the context.\"\"\"\n",
    "    \n",
    "    answer_response = chat.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": answer_prompt}],\n",
    "        temperature=0.3,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    answer = answer_response.choices[0].message.content\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  {answer}\")\n",
    "        print()\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"reasoning_trace\": trace,\n",
    "        \"retrieved_docs\": [doc['title'] for doc in unique_docs],\n",
    "        \"num_docs_retrieved\": len(unique_docs)\n",
    "    }\n",
    "\n",
    "# Test the complete agentic RAG system\n",
    "complex_question = \"How can I build an AI agent that uses search and evaluation tools effectively?\"\n",
    "print(\"=\" * 80)\n",
    "print(f\"Question: {complex_question}\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "result = advanced_agentic_rag(complex_question, verbose=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"- Retrieved {result['num_docs_retrieved']} unique documents\")\n",
    "print(f\"- Documents: {', '.join(result['retrieved_docs'])}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Agentic RAG Patterns <a id=\"advanced\"></a>\n",
    "\n",
    "### 5.1 Corrective RAG (Self-Correction)\n",
    "\n",
    "The agent can detect when retrieved information is not relevant and take corrective action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrective_rag(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Corrective RAG: Agent evaluates retrieval quality and re-retrieves if needed\n",
    "    \"\"\"\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    max_attempts = 2\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        print(f\"Attempt {attempt + 1}:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Retrieve documents\n",
    "        docs = simple_retrieve(query, top_k=3)\n",
    "        print(f\"Retrieved: {[doc['title'] for doc in docs]}\")\n",
    "        \n",
    "        # Evaluate relevance\n",
    "        context = \"\\n\".join([doc['title'] for doc in docs])\n",
    "        relevance_check = f\"\"\"Are these document titles relevant to the query: '{query}'?\n",
    "\n",
    "Documents:\n",
    "{context}\n",
    "\n",
    "Answer with 'RELEVANT' or 'NOT_RELEVANT: <reason>'\"\"\"\n",
    "        \n",
    "        relevance_response = chat.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": relevance_check}],\n",
    "            temperature=0.2,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        \n",
    "        relevance = relevance_response.choices[0].message.content\n",
    "        print(f\"Relevance Check: {relevance}\")\n",
    "        \n",
    "        if \"RELEVANT\" in relevance.upper() and \"NOT_RELEVANT\" not in relevance.upper():\n",
    "            print(\"✓ Documents are relevant, proceeding with answer generation\\n\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"✗ Documents not relevant, reformulating query...\\n\")\n",
    "            # Reformulate query for next attempt\n",
    "            reformulated = reformulate_query(query)\n",
    "            if len(reformulated) > 1:\n",
    "                query = reformulated[1]  # Try first alternative\n",
    "                print(f\"New query: {query}\\n\")\n",
    "    \n",
    "    # Generate final answer\n",
    "    context_full = \"\\n\\n\".join([f\"{doc['title']}:\\n{doc['content']}\" for doc in docs])\n",
    "    answer_response = chat.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Context:\\n{context_full}\\n\\nQuestion: {query}\\n\\nAnswer:\"}],\n",
    "        temperature=0.3,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer_response.choices[0].message.content,\n",
    "        \"attempts\": attempt + 1,\n",
    "        \"final_docs\": [doc['title'] for doc in docs]\n",
    "    }\n",
    "\n",
    "# Test corrective RAG\n",
    "print(\"=\" * 80)\n",
    "print(\"Corrective RAG Example\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "result = corrective_rag(\"What are the best practices for prompt engineering?\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Final Answer:\")\n",
    "print(result['answer'])\n",
    "print(f\"\\nAttempts: {result['attempts']}\")\n",
    "print(f\"Final Documents: {', '.join(result['final_docs'])}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Adaptive Retrieval\n",
    "\n",
    "The agent adapts its retrieval strategy based on the query type and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_retrieval(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Adaptive RAG: Agent chooses retrieval strategy based on query analysis\n",
    "    \"\"\"\n",
    "    # Analyze query to determine strategy\n",
    "    analysis_prompt = f\"\"\"Analyze this query and classify it:\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Classification:\n",
    "- FACTUAL: Simple factual question (retrieve 1-2 docs)\n",
    "- COMPARATIVE: Comparing multiple concepts (retrieve 3-4 docs)\n",
    "- COMPLEX: Multi-faceted question (retrieve 4+ docs, use decomposition)\n",
    "\n",
    "Respond with ONLY the classification (FACTUAL, COMPARATIVE, or COMPLEX)\"\"\"\n",
    "    \n",
    "    analysis_response = chat.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": analysis_prompt}],\n",
    "        temperature=0.2,\n",
    "        max_tokens=50\n",
    "    )\n",
    "    \n",
    "    query_type = analysis_response.choices[0].message.content.strip().upper()\n",
    "    print(f\"Query Type: {query_type}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Adapt retrieval based on type\n",
    "    if \"FACTUAL\" in query_type:\n",
    "        print(\"Strategy: Simple retrieval with 2 documents\")\n",
    "        docs = simple_retrieve(query, top_k=2)\n",
    "    elif \"COMPARATIVE\" in query_type:\n",
    "        print(\"Strategy: Enhanced retrieval with query reformulation\")\n",
    "        reformulated = reformulate_query(query)\n",
    "        all_docs = []\n",
    "        for rq in reformulated[:2]:\n",
    "            all_docs.extend(simple_retrieve(rq, top_k=2))\n",
    "        # Remove duplicates\n",
    "        seen = set()\n",
    "        docs = []\n",
    "        for doc in all_docs:\n",
    "            if doc['id'] not in seen:\n",
    "                docs.append(doc)\n",
    "                seen.add(doc['id'])\n",
    "    else:  # COMPLEX\n",
    "        print(\"Strategy: Query decomposition + multi-step retrieval\")\n",
    "        sub_queries = decompose_query(query)\n",
    "        all_docs = []\n",
    "        for sq in sub_queries[:2]:\n",
    "            all_docs.extend(simple_retrieve(sq, top_k=2))\n",
    "        # Remove duplicates\n",
    "        seen = set()\n",
    "        docs = []\n",
    "        for doc in all_docs:\n",
    "            if doc['id'] not in seen:\n",
    "                docs.append(doc)\n",
    "                seen.add(doc['id'])\n",
    "    \n",
    "    print(f\"Retrieved {len(docs)} documents: {[doc['title'] for doc in docs]}\")\n",
    "    print()\n",
    "    \n",
    "    # Generate answer\n",
    "    context = \"\\n\\n\".join([f\"{doc['title']}:\\n{doc['content']}\" for doc in docs])\n",
    "    answer_response = chat.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"}],\n",
    "        temperature=0.3,\n",
    "        max_tokens=400\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer_response.choices[0].message.content,\n",
    "        \"query_type\": query_type,\n",
    "        \"docs_retrieved\": len(docs),\n",
    "        \"doc_titles\": [doc['title'] for doc in docs]\n",
    "    }\n",
    "\n",
    "# Test with different query types\n",
    "queries = [\n",
    "    \"What is RAG?\",\n",
    "    \"What are the differences between Azure AI Search and vector embeddings?\",\n",
    "    \"How can I build a comprehensive AI agent system with evaluation and search capabilities?\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Query: {q}\")\n",
    "    print(\"=\" * 80)\n",
    "    result = adaptive_retrieval(q)\n",
    "    print(f\"Answer: {result['answer'][:200]}...\")\n",
    "    print(\"=\" * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Best Practices <a id=\"summary\"></a>\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**Agentic RAG Capabilities:**\n",
    "1. **Query Planning**: Break down complex questions into sub-queries\n",
    "2. **Query Reformulation**: Generate alternative phrasings for better retrieval\n",
    "3. **Self-Reflection**: Evaluate if retrieved information is sufficient\n",
    "4. **Corrective Retrieval**: Re-retrieve if initial results are poor\n",
    "5. **Adaptive Strategy**: Adjust approach based on query type\n",
    "6. **Transparent Reasoning**: Provide trace of decision-making process\n",
    "\n",
    "### Implementation Best Practices\n",
    "\n",
    "1. **Start Simple**: Begin with basic RAG, add agentic features incrementally\n",
    "2. **Use Proper Retrieval**: In production, use vector search (Azure AI Search, embeddings)\n",
    "3. **Set Iteration Limits**: Prevent infinite loops in self-reflection cycles\n",
    "4. **Monitor Costs**: Agentic RAG uses more API calls; optimize accordingly\n",
    "5. **Log Everything**: Keep detailed traces for debugging and improvement\n",
    "6. **Evaluate Performance**: Measure quality improvements vs. cost increases\n",
    "7. **Handle Failures**: Implement fallbacks when retrieval or generation fails\n",
    "8. **Optimize Prompts**: Fine-tune prompts for each agentic step\n",
    "\n",
    "### When to Use Agentic RAG\n",
    "\n",
    "**Good Use Cases:**\n",
    "- Complex multi-step questions\n",
    "- High-stakes applications requiring accuracy\n",
    "- Scenarios with ambiguous queries\n",
    "- Applications benefiting from explainability\n",
    "\n",
    "**Maybe Not Needed:**\n",
    "- Simple factual lookups\n",
    "- Cost-sensitive applications\n",
    "- Real-time, low-latency requirements\n",
    "- Well-defined, straightforward queries\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "1. **Use Vector Search**: Implement proper semantic search with embeddings\n",
    "2. **Caching**: Cache retrieved documents and intermediate results\n",
    "3. **Parallel Processing**: Run sub-queries in parallel when possible\n",
    "4. **Streaming**: Stream responses for better user experience\n",
    "5. **Monitoring**: Track success rates, iteration counts, costs\n",
    "6. **A/B Testing**: Compare agentic vs. traditional RAG performance\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Implement with real Azure AI Search and vector embeddings\n",
    "- Add tool usage (calculator, code execution, etc.)\n",
    "- Integrate with Microsoft Agent Framework\n",
    "- Build evaluation pipelines to measure improvements\n",
    "- Explore multi-agent RAG systems\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Agentic Retrieval in Azure AI Search](https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/introducing-agentic-retrieval-in-azure-ai-search/4414677)\n",
    "- [Azure AI Search Documentation](https://learn.microsoft.com/azure/search/)\n",
    "- [Microsoft Agent Framework](https://learn.microsoft.com/azure/ai-studio/)\n",
    "- [RAG Patterns and Best Practices](https://learn.microsoft.com/azure/ai-studio/concepts/retrieval-augmented-generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "Try implementing these enhancements:\n",
    "\n",
    "1. **Hybrid Search**: Combine keyword and semantic search results\n",
    "2. **Re-ranking**: Add a re-ranking step after initial retrieval\n",
    "3. **Citation Generation**: Make the agent cite specific documents in its answer\n",
    "4. **Confidence Scoring**: Have the agent rate its confidence in the answer\n",
    "5. **Multi-turn RAG**: Extend to handle follow-up questions with conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your practice code here\n",
    "# Try building one of the enhancements above!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
